\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}} % Images path

\begin{document}

\section{Introduction}

The Bag of Visual Words (BoW) model is a popular computer vision technique used
for image classification or retrieval. Inspired by the analogous model
used in natural language processing, this approach it's based on the idea of
treating images as collections of
visual words belonging to a visual vocabulary, which is obtained by clustering
local features extracted from a (possibly different) set of representative images.\\
This report presents an implementation the BoW image classifier for scene recognition by first building a visual vocabulary from a set of test images and then performing multi-class classification using K-Nearest Neighbors (KNN) and Support Vector Machines (SVM) classifiers.\\
In particular, the visual vocabulary is built by clustering SIFT descriptors
extracted from the test images using the K-Means algorithm. 
Descriptors are computed both from keypoints detected with the SIFT detector and
from ones obtained by dense sampling of the images with a fixed grid.\\
In the classification phase, the performance of the KNN classifier is compared
with that of different SVM classifiers, all using the ``\itt{one-vs-all}''
strategy with different kernels.\\
Additionally, different ways to represent images as input feature vectors are
tested. These include the classic representation as normalized histograms of
visual words, the implementation of the \itt{soft assignment} techniques
proposed by \itt{Van Gemert et al.}~\cite{gemert} and the use of the
\itt{spatial pyramid feature representation} proposed by \itt{Lazebnik et
al.}~\cite{lazebnik}.\\
The objectives of this study are to compare the performance of the different
classifiers and image representations and to reproduce the results obtained by
\itt{Van Gemert et al.}~\cite{gemert} and \itt{Lazebnik et al.}~\cite{lazebnik}
on the \itt{15-Scenes} dataset.\\
The report is organized as follows. 
Section~\ref{sec:dataset-description} presents the dataset used and analyzes the
images distribution.
Section~\ref{sec:visual-vocabulary-construction} explains how the visual
vocabulary is built from the sampled descriptors, while in
Section~\ref{sec:image-representation} the different image representation
techniques are compared.
Then, Section~\ref{sec:classification} presents the classifiers used for the
multi-class classification task, while in Section~\ref{sec:results} the results
obtained in this study are summarized, followed by some final considerations in
the last section.\\
From a technical point of view, all the classifiers have been implemented in
Python $3.12$ adopting the \ttt{scikit-learn} library~\cite{scikit-learn}
(version \ttt{1.5.2}). In particular, the SVM classifier have been implemented 
using the \ttt{SVC} class, which internally relies on
\ttt{libsvm}~\cite{libsvm}. 
However, with the aim of maintaining a clear and concise report of the work, the
following sections will only present concepts from a theoretical point of view
and the most noticeable results of the study,
while further analysis and implementation details will be available
on the author's GitHub repository~\cite{github}.


\end{document}
